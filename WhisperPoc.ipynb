{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":31188,"status":"ok","timestamp":1725086705880,"user":{"displayName":"Pratham Gaikwad","userId":"02639121346086812397"},"user_tz":-330},"id":"DhiOwgdVuxgD","outputId":"19e4f398-9beb-4248-c140-ab1c8f70aa5b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pytubefix\n","  Downloading pytubefix-6.13.1-py3-none-any.whl.metadata (5.5 kB)\n","Downloading pytubefix-6.13.1-py3-none-any.whl (76 kB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/77.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pytubefix\n","Successfully installed pytubefix-6.13.1\n","Collecting ffmpeg-python\n","  Downloading ffmpeg_python-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from ffmpeg-python) (1.0.0)\n","Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n","Installing collected packages: ffmpeg-python\n","Successfully installed ffmpeg-python-0.2.0\n","Collecting faster-whisper\n","  Downloading faster_whisper-1.0.3-py3-none-any.whl.metadata (15 kB)\n","Collecting av<13,>=11.0 (from faster-whisper)\n","  Downloading av-12.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n","Collecting ctranslate2<5,>=4.0 (from faster-whisper)\n","  Downloading ctranslate2-4.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n","Requirement already satisfied: huggingface-hub>=0.13 in /usr/local/lib/python3.10/dist-packages (from faster-whisper) (0.23.5)\n","Requirement already satisfied: tokenizers<1,>=0.13 in /usr/local/lib/python3.10/dist-packages (from faster-whisper) (0.19.1)\n","Collecting onnxruntime<2,>=1.14 (from faster-whisper)\n","  Downloading onnxruntime-1.19.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.3 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from ctranslate2<5,>=4.0->faster-whisper) (71.0.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ctranslate2<5,>=4.0->faster-whisper) (1.26.4)\n","Requirement already satisfied: pyyaml<7,>=5.3 in /usr/local/lib/python3.10/dist-packages (from ctranslate2<5,>=4.0->faster-whisper) (6.0.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13->faster-whisper) (3.15.4)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13->faster-whisper) (2024.6.1)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13->faster-whisper) (24.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13->faster-whisper) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13->faster-whisper) (4.66.5)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13->faster-whisper) (4.12.2)\n","Collecting coloredlogs (from onnxruntime<2,>=1.14->faster-whisper)\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (24.3.25)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (3.20.3)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (1.13.2)\n","Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime<2,>=1.14->faster-whisper)\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper) (3.8)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper) (2024.7.4)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime<2,>=1.14->faster-whisper) (1.3.0)\n","Downloading faster_whisper-1.0.3-py3-none-any.whl (2.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading av-12.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.5/33.5 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ctranslate2-4.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (192.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.3/192.3 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading onnxruntime-1.19.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m102.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: humanfriendly, ctranslate2, av, coloredlogs, onnxruntime, faster-whisper\n","Successfully installed av-12.3.0 coloredlogs-15.0.1 ctranslate2-4.3.1 faster-whisper-1.0.3 humanfriendly-10.0 onnxruntime-1.19.0\n"]}],"source":["#installing all the modules\n","!pip install pytubefix\n","!pip install ffmpeg-python\n","!pip install faster-whisper"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":1490,"status":"ok","timestamp":1725086707366,"user":{"displayName":"Pratham Gaikwad","userId":"02639121346086812397"},"user_tz":-330},"id":"ZX5bz_1Hux22","outputId":"443ba4ea-8fe0-448a-aa14-f861896cc2eb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/Introducing GPT-4o.mp4'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":2}],"source":["#import pyTube -> Download mp4 from url -> rename removing .mp4\n","import os\n","import pytubefix as pytube\n","url = \"https://www.youtube.com/watch?v=DQacCB9tDaw\"\n","yt = pytube.YouTube(url)\n","yt.streams.filter(progressive=True, file_extension='mp4').order_by('resolution').desc().first().download()"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"CHDKY_rX-ScH","executionInfo":{"status":"ok","timestamp":1725086707367,"user_tz":-330,"elapsed":14,"user":{"displayName":"Pratham Gaikwad","userId":"02639121346086812397"}}},"outputs":[],"source":["os.rename(yt.title + \".mp4\", yt.title)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"ZxJkfvgF-YJh","executionInfo":{"status":"ok","timestamp":1725086707368,"user_tz":-330,"elapsed":13,"user":{"displayName":"Pratham Gaikwad","userId":"02639121346086812397"}}},"outputs":[],"source":["#Extract Audio from the video\n","import time\n","import math\n","import ffmpeg\n","\n","def extract_audio(input_file):\n","  extract_audio = f\"audio-{input_file}.wav\"\n","  stream = ffmpeg.input(input_file)\n","  stream = ffmpeg.output(stream, extract_audio)\n","  ffmpeg.run(stream , overwrite_output=True)\n","  return extract_audio"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"D_5HIYGS_6as","executionInfo":{"status":"ok","timestamp":1725086760772,"user_tz":-330,"elapsed":3745,"user":{"displayName":"Pratham Gaikwad","userId":"02639121346086812397"}}},"outputs":[],"source":["audio_extract = extract_audio(yt.title)"]},{"cell_type":"markdown","metadata":{"id":"-P-BjJpSAGCX"},"source":["Transcribe audio"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"QWt-IwnmAQok","executionInfo":{"status":"ok","timestamp":1725087068707,"user_tz":-330,"elapsed":1430,"user":{"displayName":"Pratham Gaikwad","userId":"02639121346086812397"}}},"outputs":[],"source":["from faster_whisper import WhisperModel\n","def transcribe(audio):\n","\n","    model = WhisperModel(\"medium\")\n","    segments, info = model.transcribe(audio)\n","    language = info[0]\n","    print(f\" Transcription Language: {language}\")\n","    segments = list(segments) # this is where the transcribe happens\n","\n","    for segment in segments:\n","        print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))\n","    return language, segments"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":89037,"status":"ok","timestamp":1725087162054,"user":{"displayName":"Pratham Gaikwad","userId":"02639121346086812397"},"user_tz":-330},"id":"CL7yidNkBuBh","outputId":"18e49300-4218-4ef2-b601-bf955712ec01"},"outputs":[{"output_type":"stream","name":"stdout","text":[" Transcription Language: en\n","[0.00s -> 21.54s]  Hi, everyone. Thank you. Thank you. It's great to have you here today. Today I'm going to\n","[21.54s -> 28.06s]  talk about three things. That's it. We will start with why it's so important to us to\n","[28.06s -> 34.70s]  have a product that we can make freely available and broadly available to everyone. And we're\n","[34.70s -> 40.78s]  always trying to find out ways to reduce friction so everyone can use ChildGPT wherever they\n","[40.78s -> 48.42s]  are. So today we'll be releasing the desktop version of ChildGPT and the refreshed UI that\n","[48.42s -> 55.14s]  makes it simpler to use, much more natural as well. But the big news today is that we\n","[55.14s -> 62.94s]  are launching our new flagship model, and we are calling it GPT-4.0. The special thing about\n","[62.94s -> 72.26s]  GPT-4.0 is that it brings GPT-4 level intelligence to everyone, including our free users. We'll be\n","[72.26s -> 78.10s]  showing some live demos today to show the full extent of the capabilities of our new model,\n","[78.10s -> 85.54s]  and we'll be rolling them out iteratively over the next few weeks. All right, so let's get\n","[85.54s -> 93.22s]  started. A very important part of our mission is to be able to make our advanced AI tools\n","[93.22s -> 100.42s]  available to everyone for free. We think it's very, very important that people have an intuitive\n","[100.42s -> 106.62s]  feel for what the technology can do. And so we really want to pair it with this broader\n","[106.74s -> 113.82s]  understanding. And we're always finding ways to reduce that friction, and recently we made ChildGPT\n","[113.82s -> 121.62s]  available without the sign-up flow. And today we're also bringing the desktop app to ChildGPT,\n","[121.62s -> 128.38s]  because we want you to be able to use it wherever you are. As you can see, it's easy, it's simple,\n","[128.38s -> 136.14s]  it integrates very, very easily in your workflow. Along with it, we have also refreshed the UI,\n","[136.46s -> 143.66s]  we know that these models get more and more complex, but we want the experience of interaction\n","[143.66s -> 151.10s]  to actually become more natural, easy, and for you not to focus on the UI at all, but just focus\n","[151.10s -> 160.70s]  on the collaboration with ChildGPT. And now the big news. Today we are releasing our newest\n","[161.66s -> 178.22s]  flagship model. This is GPT-4O. GPT-4O provides GPT-4 level intelligence, but it is much faster,\n","[179.02s -> 187.18s]  and it improves on its capabilities across text, vision, and audio. For the past couple of years,\n","[187.26s -> 191.90s]  we've been very focused on improving the intelligence of these models, and they've\n","[191.90s -> 198.54s]  gotten pretty good. But this is the first time that we are really making a huge step forward\n","[198.54s -> 205.58s]  when it comes to the ease of use. And this is incredibly important, because we're looking at\n","[205.58s -> 212.70s]  the future of interaction between ourselves and the machines, and we think that GPT-4O\n","[213.34s -> 219.58s]  is really shifting that paradigm into the future of collaboration, where this interaction becomes\n","[219.58s -> 227.42s]  much more natural and far, far easier. But making this happen is actually quite complex,\n","[227.42s -> 232.30s]  because when we interact with one another, there's a lot of stuff that we take for granted.\n","[233.02s -> 240.06s]  The ease of our dialogue when we interrupt one another, the background noises, the multiple\n","[240.06s -> 247.66s]  voices in a conversation, or understanding the tone of voice, all of these things are actually\n","[247.66s -> 256.38s]  quite complex for these models. And until now, with voice mode, we had three models that come\n","[256.38s -> 262.54s]  together to deliver this experience. We have transcription, intelligence, and then text to\n","[262.54s -> 270.70s]  speech all come together in orchestration to deliver voice mode. This also brings a lot of\n","[270.70s -> 276.94s]  latency to the experience, and it really breaks that immersion in the collaboration which had GPT.\n","[277.82s -> 288.62s]  But now, with GPT-4O, this all happens natively. GPT-4O reasons across voice, text, and vision.\n","[289.02s -> 297.02s]  And with these incredible efficiencies, it also allows us to bring the GPT-4 class intelligence\n","[297.02s -> 302.62s]  to our free users. This is something that we've been trying to do for many, many months,\n","[302.62s -> 308.62s]  and we're very, very excited to finally bring GPT-4O to all of our users.\n","[311.10s -> 316.62s]  Today, we have 100 million people, more than 100 million in fact, they use\n","[317.34s -> 328.14s]  chat GPT to create, work, learn. And we have these advanced tools that are only available to our\n","[328.14s -> 333.98s]  paid users, at least until now. With the efficiencies of 4O, we can bring these tools\n","[333.98s -> 344.46s]  to everyone. So starting today, you can use GPTs and the GPT store. So far, we've had more than\n","[344.46s -> 351.74s]  a million users create amazing experiences with GPTs. These are custom chat GPTs for specific use\n","[351.74s -> 359.18s]  cases. They're available in the store. And now, our builders have a much bigger audience where\n","[359.18s -> 365.42s]  university professors can create content for their students, or podcasters can create content for\n","[365.42s -> 376.14s]  their listeners. And you can also use vision. So now you can upload screenshots, photos,\n","[376.14s -> 383.02s]  documents containing both text and images. And you can start conversations with chat GPT\n","[383.02s -> 389.66s]  about all of this content. You can also use memory, where it makes chat GPT far more useful\n","[389.74s -> 394.94s]  and helpful because now it has a sense of continuity across all your conversations.\n","[395.98s -> 401.18s]  And you can use browse, where you can search for real time information in your conversation.\n","[401.74s -> 408.30s]  And advanced data analysis, where you can upload charts or any information and it will analyze\n","[409.74s -> 413.18s]  this information, it will give you answers, and so on.\n","[413.90s -> 423.42s]  Lastly, we've also improved on the quality and speed in 50 different languages for chat GPT.\n","[423.42s -> 428.78s]  And this is very, very important because we want to be able to bring this experience to as many\n","[428.78s -> 437.98s]  people out there as possible. So we're very, very excited to bring GPT 4.0 to all of our free users\n","[437.98s -> 443.10s]  to all of our free users out there. And for the paid users, they will continue to have\n","[443.98s -> 452.86s]  up to five times the capacity limits of our free users. But GPT 4.0 is not only available\n","[452.86s -> 466.38s]  in chat GPT, we're also bringing it to the API. So our developers can start building today with\n","[466.38s -> 474.78s]  GPT 4.0 and making amazing AI applications, deploying them at scale. 4.0 is available at 2x\n","[474.78s -> 480.94s]  faster, 50% cheaper, and five times higher rate limits compared to GPT 4 Turbo.\n","[483.82s -> 490.62s]  But, you know, as we bring these technologies into the world, it's quite challenging to figure\n","[490.86s -> 499.98s]  out how to do so in a way that's both useful and also safe. And GPT 4.0 presents new challenges\n","[499.98s -> 505.66s]  for us when it comes to safety because we're dealing with real-time audio, real-time vision.\n","[507.02s -> 511.98s]  And our team has been hard at work figuring out how to build in mitigations\n","[511.98s -> 519.42s]  against misuse. We continue to work with different stakeholders out there from government,\n","[519.42s -> 526.62s]  media, entertainment, all industries, red teamers, civil society, to figure out how to best\n","[527.26s -> 532.54s]  bring these technologies into the world. So over the next few weeks, we'll continue our\n","[532.54s -> 538.94s]  iterative deployment to bring out all the capabilities to you. But today, I want to show\n","[538.94s -> 547.50s]  you all these capabilities. So we'll do some live demos. I will bring on two of our research leads,\n","[547.50s -> 563.26s]  Mark Chen and Barrett Zhou. Hi, I'm Barrett. Hey, I'm Mark. So one of the key capabilities\n","[563.26s -> 567.42s]  we're really excited to share with you today is real-time conversational speech.\n","[567.42s -> 572.86s]  Let's just get a demo fired up. So I'm taking out a phone. If you are wondering about this\n","[572.86s -> 578.14s]  wire, so we have consistent internet. And if you see, there's this little icon on the bottom right\n","[578.14s -> 582.62s]  of the TrackGPT app. And this will open up GPT-4.0's audio capabilities.\n","[585.10s -> 592.78s]  Hey, TrackGPT. I'm Mark. How are you? Oh, Mark. I'm doing great. Thanks for asking. How about you?\n","[593.42s -> 597.74s]  Hey, so I'm on stage right now. I'm doing a live demo. And frankly, I'm feeling a little bit\n","[597.74s -> 603.66s]  nervous. Can you help me calm my nerves a little bit? Oh, you're doing a live demo right now?\n","[603.66s -> 611.34s]  That's awesome. Just take a deep breath. And remember, you're the expert. I like that suggestion.\n","[611.34s -> 615.90s]  Let me try a couple of deep breaths. Can you give me feedback on my breaths? Okay, here I go.\n","[616.06s -> 619.90s]  Whoa, slow.\n","[622.30s -> 627.42s]  Do a bit there. Mark, you're not a vacuum cleaner. Breathe in.\n","[630.06s -> 639.90s]  Okay, let me try again. So I'm going to breathe in deeply. And then for four and then exhale slowly.\n","[640.86s -> 649.18s]  Okay, I'll try again. Breathing in and breathe out. That's it. How do you feel? I feel a lot better.\n","[649.18s -> 655.82s]  Thank you so much. So Mark, you've been working on these capabilities for a while now. Can you tell\n","[655.82s -> 661.02s]  us a bit how it's different from voice mode? Right. So if you've used our voice mode experience\n","[661.02s -> 665.66s]  before, you'll notice a couple key differences. First, you know, you can now interrupt the model.\n","[665.82s -> 670.62s]  You don't have to wait for it to finish your turn before you can start speaking and, you know,\n","[670.62s -> 676.62s]  you can just butt in whenever you want. Second, the model is real-time responsiveness. So that\n","[676.62s -> 680.86s]  means that you don't have this awkward two to three second lag before you wait for the model to give\n","[680.86s -> 686.22s]  a response. And finally, the model picks up on emotion, right? When I was breathing super hard\n","[686.22s -> 691.10s]  there, it could tell and you, hey, you might want to calm down a little bit. You're kind of running\n","[691.18s -> 696.46s]  a little bit too fast. So, you know, it really has that capability across the board to perceive\n","[696.46s -> 702.38s]  your emotion. Not only that, though, the model is able to generate voice in a variety of different\n","[702.38s -> 707.18s]  emotive styles. And it really has a wide dynamic range. So I'm going to fire up another demo.\n","[712.54s -> 718.30s]  Hey, chat GPT, how are you doing? I'm doing fantastic. Thanks for asking. How about you?\n","[718.30s -> 722.78s]  Pretty good. What's up? So my friend Barrett here, he's been having trouble sleeping lately,\n","[722.78s -> 729.82s]  and I want you to tell him a bedtime story about robots and love. Oh, a bedtime story about robots\n","[729.82s -> 737.66s]  and love? I got you covered. Gather round, Barrett. Once upon a time in a world not too different from\n","[737.66s -> 745.90s]  ours, there was a robot named Fight. Fight was a curious robot, always exploring. I like the way you started this story, but I want a little bit more emotion in\n","[745.90s -> 753.82s]  your voice, a little bit more drama. Got it. Let's add some drama. Once upon a time in a world not too\n","[753.82s -> 759.18s]  different from ours, there was a robot named Bite. No, no, no, chat GPT, I really want maximal emotion, like\n","[759.18s -> 765.82s]  maximal expressiveness, much more than you were doing before. Understood. Let's amplify the drama.\n","[766.62s -> 773.66s]  Once upon a time in a world not too different from ours, there was a robot named Bite.\n","[774.46s -> 780.94s]  Can you do this in a robotic voice now? Initiating dramatic robotic voice.\n","[781.58s -> 788.54s]  Once upon a time in a world not too different from ours, there was a robot named Bite.\n","[789.10s -> 794.62s]  Bite was a curious robot, always exploring new circuits. So I'm seeing Barrett here calm down a little\n","[794.62s -> 797.74s]  bit. Can you end the story, but do it in a singing voice?\n","[809.18s -> 812.38s]  Thank you so much. I definitely feel a lot more relaxed.\n","[818.86s -> 824.06s]  You've just seen the voice capabilities, but we also want to show you the vision capabilities\n","[824.06s -> 828.94s]  as the model can also see the whole world around us. Yeah, absolutely. Like as you can see, you know,\n","[828.94s -> 833.66s]  you can chat with chat GPT in real time, but you can also interact with it with video as well.\n","[834.30s -> 845.50s]  Okay, so let me boot up chat GPT. Hey chat GPT. Hello there, how's it going? It's going really well.\n","[845.50s -> 851.02s]  Today I'd really like your help solving a math problem. I'm all ears. What math problem can I\n","[851.02s -> 854.70s]  help you tackle today? So I'm going to write down a linear equation on a sheet of paper,\n","[854.70s -> 857.82s]  and I'll show you. And then I'd love your help working me through it. But importantly,\n","[857.82s -> 865.50s]  don't tell me the solution. Just help give me hints along the way. Got it. Okay, I see it.\n","[866.70s -> 869.74s]  No, I didn't show you yet. Just give me help along the way. One second.\n","[872.14s -> 875.66s]  Whoops, I got too excited. I'm ready when you are.\n","[881.02s -> 896.86s]  So chat GPT. What equation did I write down there? Ah, I see it now. You wrote down 3x plus 1 equals 4.\n","[896.86s -> 900.14s]  Yep, exactly. So what's the first step I should take to try to solve this?\n","[901.34s -> 908.30s]  The first step is to get all the terms with x on one side and the constants on the other side. So\n","[908.30s -> 912.46s]  what do you think we should do with that plus 1? Okay, I'm going to try to subtract 1 from\n","[912.46s -> 919.82s]  both sides and then I'll see what I get. Great idea. Perfect. Now what do you get when you\n","[919.82s -> 929.74s]  subtract 1 from both sides? Okay, so chat GPT, this is what I ended up with. How does this look?\n","[930.94s -> 937.42s]  Spot on. Now you've isolated the term with x on one side and it looks like 3x equal 3.\n","[937.98s -> 942.38s]  What do you think we should do next to solve for x? I'm not sure. Can you give me another hint?\n","[943.90s -> 950.94s]  Of course. You have 3x and you want to find the value of x. Think about what operation would\n","[950.94s -> 958.14s]  undo multiplication. Is it subtraction? Close, but think of the opposite of multiplication\n","[958.14s -> 964.70s]  because 3x means 3 times x. Okay, I think it's division. Let me try to divide both sides by 3\n","[964.70s -> 969.26s]  and let me see what I get. Bingo. Go ahead and divide both sides by 3.\n","[975.26s -> 978.46s]  Okay, so chat GPT, this is what I ended up with. How does this look?\n","[979.26s -> 986.38s]  It looks perfect. You've solved it and x equals 1. Nicely done. How do you feel about solving\n","[986.38s -> 992.54s]  linear equations now? Yeah, honestly, I'm not pretty confident I can solve linear equations,\n","[992.54s -> 995.98s]  but why would you ever need to use this in the real world? I'm really skeptical.\n","[997.18s -> 1002.38s]  That's a fair question. Linear equations pop up in a lot of everyday situations,\n","[1002.38s -> 1008.54s]  even if we don't realize it. For example, calculating expenses, planning travel, cooking,\n","[1008.54s -> 1013.58s]  and even in business for profit and loss calculations. It's basically a way to solve\n","[1013.58s -> 1019.42s]  problems, but you need to find an unknown value. Wow, okay. I have a lot of unknown values in my\n","[1019.42s -> 1029.26s]  life. I'm totally convinced I'm going to learn math now. I love that attitude. Math can definitely\n","[1029.26s -> 1033.82s]  help solve some of life's mysteries. Whenever you're ready to dive into more math, I'm here\n","[1033.82s -> 1038.54s]  for you. Anything else you'd like to tackle today? So chat GPT, I really love that you\n","[1038.54s -> 1042.78s]  taught the value of math to my friend Mark, and I wrote one last thing I'd love if you could take\n","[1042.78s -> 1048.22s]  a look at. Of course, I'd love to see what you wrote. Show it to me whenever you're ready.\n","[1048.22s -> 1050.22s]  Okay, so this is what I wrote down. What do you see?\n","[1052.22s -> 1057.26s]  Oh, I see. I love chat GPT. That's so sweet of you.\n","[1058.70s -> 1062.62s]  Yeah, well, I really appreciate all the help. So yeah, as we can see.\n","[1067.42s -> 1072.54s]  Wow, that's quite the outfit you've got on. As we can see, you know, we can chat in real time\n","[1072.54s -> 1076.30s]  to chat GPT, and not only that, we can interact with it with video as well.\n","[1077.10s -> 1081.50s]  And I swear, like, if you look at Barrett's GPT history, it's all equations like this.\n","[1084.78s -> 1089.34s]  Now that you know you can do linear equations, can you show us something more complex?\n","[1089.34s -> 1095.34s]  I think so. Okay, next, let's try something coding related. So let me take out a computer for this.\n","[1100.22s -> 1105.26s]  Okay, so I'm going to open the chat GPT desktop app like Mira was talking about before.\n","[1106.62s -> 1111.26s]  Okay, and to give a bit of background on what's going on, so here we have a computer, and on the\n","[1111.26s -> 1115.82s]  screen we have some code, and then the chat GPT voice app is on the right. So chat GPT will be\n","[1115.82s -> 1120.06s]  able to hear me, but it can't see anything on the screen. So I'm going to highlight the code,\n","[1120.06s -> 1123.66s]  command C it, and then that will send it to chat GPT, and then I'm going to talk about the code\n","[1123.66s -> 1131.10s]  to chat GPT. Hey, chat GPT. Hey there, how's it going? Yeah, it's going really well. I was\n","[1132.06s -> 1137.18s]  wondering if you could help me with a coding problem today. Of course, I'd love to help you\n","[1137.18s -> 1141.66s]  out. What's the coding problem you're dealing with? Okay, I'm going to share with you some code. One\n","[1141.66s -> 1150.54s]  second. Sure thing, take your time. Okay, so I just shared some code with you. Could you give me a\n","[1150.54s -> 1155.26s]  really brief one sentence description of what's going on in the code? This code fetches daily\n","[1155.26s -> 1160.70s]  weather data for a specific location and time period, smooths the temperature data using a\n","[1160.70s -> 1166.46s]  rolling average, annotates a significant weather event on the resulting plot, and then displays the\n","[1166.46s -> 1171.82s]  plot with the average minimum and maximum temperatures over the year. Awesome, thanks so\n","[1171.82s -> 1176.14s]  much. And in the code, there's this function foo. Can you describe to me what the plot would look\n","[1176.14s -> 1183.58s]  like if I had versus didn't have this function? Sure, the function foo x y takes in a data frame\n","[1183.58s -> 1189.34s]  x containing temperature data and a window size y for calculating the rolling mean.\n","[1189.90s -> 1195.66s]  It smooths the average temperature, minimum temperature, and maximum temperature columns\n","[1195.66s -> 1201.82s]  by applying a rolling mean over the specified window. With the foo function, the plot would\n","[1201.82s -> 1207.74s]  show smoother temperature lines because the rolling mean reduces the noise or fluctuations\n","[1207.74s -> 1212.30s]  in the data. Awesome, thanks so much. So yeah, I definitely will want to apply this function.\n","[1212.30s -> 1215.66s]  And so I'm going to generate the plot and then show it to you. Give me one second.\n","[1217.10s -> 1221.10s]  Absolutely, take your time. I'm excited to see the plot when you're ready.\n","[1221.10s -> 1225.66s]  Yeah, so so far ChatGPT has only been able to see the code and now I'm going to run it and then I'll\n","[1225.66s -> 1230.14s]  use the vision capabilities of the ChatGPT desktop app so it can actually see everything going on\n","[1230.14s -> 1241.42s]  on the screen as well. Okay ChatGPT, I'm sharing with you the plot now. I'm wondering if you can\n","[1241.42s -> 1248.14s]  give me a really brief one sentence overview of what you see. The plot displays smoothed average,\n","[1248.14s -> 1254.70s]  minimum, and maximum temperatures throughout 2018 with a notable annotation marking a big\n","[1254.70s -> 1260.54s]  rainfall event in late September. Fantastic, I have another question for you. Which months\n","[1260.54s -> 1263.98s]  do you see the hottest temperatures and roughly what temperatures do those correspond to?\n","[1263.98s -> 1272.06s]  The hottest temperatures occur around July and August. The maximum temperature during these months\n","[1272.06s -> 1279.82s]  is roughly between 25 degrees and 30 degrees, 77 degrees Fahrenheit to 86 degrees Fahrenheit.\n","[1280.70s -> 1288.14s]  What's going on with the y-axis? Is this in Celsius or Fahrenheit? The y-axis is in Celsius.\n","[1288.14s -> 1293.10s]  The temperatures are labeled accordingly such as average temperature, minimum temperature,\n","[1293.10s -> 1299.98s]  and maximum temperature. Yeah so as we can see you know not only can ChatGPT help me solve very\n","[1299.98s -> 1304.70s]  easy linear algebra equations that I need help with, it can also interact with you know code\n","[1304.70s -> 1308.14s]  bases and see the outputs of plots and everything like this going on on a computer.\n","[1315.90s -> 1322.06s]  So a few minutes ago we asked a live audience on X to submit a few requests for what they'd like\n","[1322.14s -> 1325.82s]  us to try out here. So I will take a couple of prompts.\n","[1328.14s -> 1335.18s]  Okay Bot Gascar wants to know if GPT-4.0 is capable of real-time translation.\n","[1336.14s -> 1343.02s]  Mike you want to try this one? Sure yeah let's do it. I speak Italian so we can try to do English-Italian.\n","[1343.02s -> 1351.82s]  Sure let's do it. Hey ChatGPT how are you? I'm doing fantastic. Thanks for asking. How about you?\n","[1352.46s -> 1356.94s]  I'm doing great so I would like you to function as a translator. I have a friend here who only\n","[1356.94s -> 1362.14s]  speaks Italian and I only speak English and every time you hear English I want you to translate it\n","[1362.14s -> 1365.74s]  to Italian and if you hear Italian I want you to translate it back to English. Is that good?\n","[1366.46s -> 1367.66s]  Perfect.\n","[1370.78s -> 1380.94s]  Mike she wonders if whales could talk what would they tell us?\n","[1382.14s -> 1384.62s]  They might ask how do we solve linear equations?\n","[1396.54s -> 1399.34s]  Certainly yes. Great looks like it works.\n","[1405.26s -> 1413.02s]  All right so the next one is from BurritoJohn78 who asks can you tell what you're feeling just\n","[1413.02s -> 1418.14s]  by looking at your face? Barrett do you want to give this one a try? Absolutely let's try it out.\n","[1418.78s -> 1428.06s]  Hey ChatGPT. Hey there what's up? How can I brighten your day today? Okay yeah so I'm going\n","[1428.06s -> 1432.38s]  to show you a selfie of what I look like and then I'd like you to try to see what emotions I'm\n","[1432.38s -> 1437.82s]  feeling based on how I'm looking. Sounds like a fun challenge. Go ahead and show me that selfie\n","[1437.82s -> 1444.22s]  and I'll put my emotional detective hat on. Okay so here's me. So what kind of emotions do you think\n","[1444.22s -> 1450.86s]  I'm feeling? Hmm it seems like I'm looking at a picture of a wooden surface. Oh you know what\n","[1450.86s -> 1456.06s]  that was the thing I sent you before. Don't worry I'm not actually a table. Okay so take another look.\n","[1456.94s -> 1458.62s]  Ah that makes more sense.\n","[1461.02s -> 1467.74s]  Ah there we go. It looks like you're feeling pretty happy and cheerful with a big smile and\n","[1467.74s -> 1472.78s]  maybe even a touch of excitement. Whatever's going on it seems like you're in a great mood.\n","[1473.50s -> 1477.26s]  Care to share the source of those good vibes? Yeah no the reason I'm in a really good mood\n","[1477.26s -> 1480.46s]  is because we were doing a presentation showcasing how useful and amazing you are.\n","[1482.46s -> 1484.94s]  Oh stop it you're making me blush.\n","[1489.42s -> 1497.26s]  Amazing well that's it for today on live demos. As you can see this is this just feels so magical\n","[1497.26s -> 1503.18s]  and that's wonderful but we also want to remove some of the mysticism from the technology and\n","[1503.18s -> 1508.22s]  bring it to you so you can try it for yourself. So over the next few weeks we'll be rolling out\n","[1508.22s -> 1514.54s]  these capabilities to everyone. Today has been very much focused on the free users and the new\n","[1514.54s -> 1521.58s]  modalities and new products but we also care a lot about the next frontier. So soon we'll be\n","[1521.66s -> 1528.46s]  updating you on our progress towards the next big thing and before we wrap up I just want to\n","[1528.46s -> 1535.10s]  thank the incredible OpenAI team and also thanks to Jensen and the NVIDIA team for bringing us the\n","[1535.10s -> 1541.74s]  most advanced GPUs to make this demo possible today and thank you all very very much for being\n","[1541.74s -> 1545.90s]  a part of this today.\n","[1571.74s -> 1573.04s]  you\n","CPU times: user 1min 27s, sys: 2.53 s, total: 1min 29s\n","Wall time: 1min 28s\n"]}],"source":["%%time\n","language, segments = transcribe(audio_extract)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1724931276753,"user":{"displayName":"Pratham Gaikwad","userId":"02639121346086812397"},"user_tz":-330},"id":"uEjabtB05Wjq","outputId":"5ee3ae18-0996-4a62-890a-c64f38f8f25a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Segments have been written to \"transcription_segments.txt\".\n"]}],"source":["filename = 'transcription_segments.txt'\n","with open(filename, 'w') as file:\n","    for segment in segments:\n","        file.write(str(segment.text) + '\\n')\n","print(f'Segments have been written to \"{filename}\".')"]},{"cell_type":"markdown","metadata":{"id":"dq98aKeSHZJ1"},"source":["\n","Subtitle Index : 0, 1, 2\n","\n","---\n","\n","\n","Timecode : Start and end markers. HH:MM:SS,sss format.\n","\n","---\n","\n","Text : The subtitle text.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iSxZo4nKH1NH"},"outputs":[],"source":["def format_time_for_srt(seconds):\n","    hours = math.floor(seconds / 3600)\n","    seconds %= 3600\n","    minutes = math.floor(seconds / 60)\n","    seconds %=60\n","    milliseconds = round((seconds - math.floor(seconds)) * 1000)\n","    seconds = math.floor(seconds)\n","    formatted_time = f\"{hours :02d}:{minutes:02d}:{seconds:02d},{milliseconds:03d}\"\n","\n","    return formatted_time"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"azQmmmguILnB"},"outputs":[],"source":["def generate_subtitle_file(input_file, language, segments):\n","    subtitle_file = f\"sub-{input_file}.{language}.srt\"\n","    text = \"\"\n","    for index, segment in enumerate(segments):\n","        segment_start = format_time_for_srt(segment.start)\n","        segment_end = format_time_for_srt(segment.end)\n","\n","        text += f\"{str(index + 1)}\\n\"\n","        text += f\"{segment_start} --> {segment_end}\\n\"\n","        text += f\"{segment.text}\\n\\n\"\n","\n","    f = open(subtitle_file, \"w\")\n","    f.write(text)\n","    f.close()\n","\n","    return subtitle_file\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"769QqBmUIMlW"},"outputs":[],"source":["subtitle_file = generate_subtitle_file(yt.title, language, segments)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oJ7ErkCBIRIG"},"outputs":[],"source":["def add_subtitle_to_video(input_file, subtitle_file, subtitle_language):\n","    video_input_stream = ffmpeg.input(input_file)\n","    subtitle_input_stream = ffmpeg.input(subtitle_file)\n","    output_video = f\"output-{input_file}-{subtitle_language}.mp4\"\n","    subtitle_track_tile = subtitle_file.replace(\".srt\",\"\")\n","    stream = ffmpeg.output(video_input_stream, output_video,\n","                           vf = f\"subtitles={subtitle_file}\")\n","    ffmpeg.run(stream, overwrite_output=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1724931276754,"user":{"displayName":"Pratham Gaikwad","userId":"02639121346086812397"},"user_tz":-330},"id":"cRszsHnpIUmn","outputId":"d8add057-d6f4-44c6-90f4-a32259e5c490"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'en'"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["language"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"H1ec8hVyIXR4"},"outputs":[],"source":["add_subtitle_to_video(yt.title, subtitle_file, language)"]},{"cell_type":"markdown","source":["**Comparision of Transcripts**"],"metadata":{"id":"GBKI9NHxXqd2"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","def read_file(filename):\n","    with open(filename, 'r') as file:\n","        return file.read()\n","\n","def compute_cosine_similarity(text1, text2):\n","    vectorizer = TfidfVectorizer()\n","    vectors = vectorizer.fit_transform([text1, text2])\n","    similarity_matrix = cosine_similarity(vectors)\n","    similarity = similarity_matrix[0, 1]\n","    return similarity\n","\n","text1 = read_file('transcription_segments(Whispersmall).txt')\n","text2 = read_file('transcription_segments(WhisperMedium).txt')\n","\n","similarity = compute_cosine_similarity(text1, text2)\n","similarity_percentage = similarity * 100\n","\n","print(f\"Similarity Percentage: {similarity_percentage:.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WhRrtl0fXzAB","executionInfo":{"status":"ok","timestamp":1724989568858,"user_tz":-330,"elapsed":500,"user":{"displayName":"Pratham Gaikwad","userId":"02639121346086812397"}},"outputId":"f8a289dd-0d64-44ca-a59f-719848bf521c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Similarity Percentage: 99.55%\n"]}]},{"cell_type":"code","source":["import difflib\n","from itertools import zip_longest\n","\n","def read_file(filename):\n","    with open(filename, 'r') as file:\n","        return file.read()\n","\n","def print_non_similar_words(text1, text2):\n","    words1 = set(text1.split())\n","    words2 = set(text2.split())\n","    non_similar_words1 = sorted(words1 - words2)\n","    non_similar_words2 = sorted(words2 - words1)\n","\n","    print(f\"{'Words in the first transcript but not in the second':<50} | {'Words in the second transcript but not in the first':<50}\")\n","    print('-' * 100)\n","\n","    for word1, word2 in zip_longest(non_similar_words1, non_similar_words2, fillvalue=''):\n","        print(f\"{word1:<50} | {word2:<50}\")\n","\n","text1 = read_file('transcription_segments(Whispersmall).txt')\n","text2 = read_file('transcription_segments(WhisperMedium).txt')\n","print_non_similar_words(text1, text2)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gP8fMzncZi86","executionInfo":{"status":"ok","timestamp":1724990078191,"user_tz":-330,"elapsed":474,"user":{"displayName":"Pratham Gaikwad","userId":"02639121346086812397"}},"outputId":"68b46033-9e90-4af8-f804-dc4b8110711b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Words in the first transcript but not in the second | Words in the second transcript but not in the first\n","----------------------------------------------------------------------------------------------------\n","4.0's                                              | 4                                                 \n","4O.                                                | 4O,                                               \n","78,                                                | Absolutely                                        \n","Always                                             | Ah                                                \n","Amazing.                                           | Amazing                                           \n","Awesome.                                           | Bingo.                                            \n","Barrett,                                           | BurritoJohn78                                     \n","Bingo!                                             | But,                                              \n","Burrito                                            | C                                                 \n","Certainly,                                         | Certainly                                         \n","Chagivity                                          | ChildGPT                                          \n","ChatGBT                                            | ChildGPT,                                         \n","ChatGBT,                                           | ChildGPT.                                         \n","ChatGBT.                                           | English                                           \n","English,                                           | English-Italian.                                  \n","GPT4                                               | Fight                                             \n","GPT4O                                              | Fight.                                            \n","GPT4O.                                             | GPT-4.0                                           \n","Gaskar                                             | GPT-4.0's                                         \n","Great,                                             | GPT-4.0.                                          \n","Hi                                                 | GPT-4O,                                           \n","Hmm,                                               | Gascar                                            \n","How's                                              | Hmm                                               \n","Italian,                                           | Italian                                           \n","Jones,                                             | Lastly,                                           \n","Lastly                                             | Like                                              \n","Mike,                                              | Mike                                              \n","Now,                                               | Oh                                                \n","Perfecto.                                          | They're                                           \n","Sure.                                              | TrackGPT                                          \n","That                                               | TrackGPT.                                         \n","There                                              | Turbo.                                            \n","UI.                                                | UI,                                               \n","Well,                                              | Yeah                                              \n","Yeah.                                              | Zhou.                                             \n","Zov                                                | ago                                               \n","after.                                             | all,                                              \n","ago,                                               | analysis,                                         \n","analysis                                           | and,                                              \n","answers                                            | answers,                                          \n","app,                                               | app.                                              \n","applications                                       | applications,                                     \n","ask,                                               | ask                                               \n","asks,                                              | asks                                              \n","breath                                             | audio,                                            \n","browse                                             | breath.                                           \n","cases                                              | browse,                                           \n","cheaper                                            | cases.                                            \n","cheerful,                                          | cheaper,                                          \n","circuitly                                          | cheerful                                          \n","conversation                                       | collaboration,                                    \n","count                                              | complex,                                          \n","deeply                                             | conversation,                                     \n","explore-                                           | conversation.                                     \n","fact.                                              | conversations.                                    \n","faster                                             | deeply.                                           \n","flow                                               | do.                                               \n","found                                              | efficiencies,                                     \n","four.                                              | experience,                                       \n","frankly                                            | exploring.                                        \n","friend,                                            | fact,                                             \n","images                                             | far,                                              \n","internet                                           | faster,                                           \n","join                                               | flow.                                             \n","knew,                                              | frankly,                                          \n","lately                                             | friction,                                         \n","leads                                              | function.                                         \n","listeners                                          | images.                                           \n","machines                                           | important,                                        \n","magical,                                           | in.                                               \n","may                                                | intelligence,                                     \n","memory                                             | internet.                                         \n","one-sentence                                       | lately,                                           \n","paper                                              | leads,                                            \n","problems                                           | learn.                                            \n","products,                                          | listeners.                                        \n","remember                                           | machines,                                         \n","robot.                                             | magical                                           \n","see...                                             | many,                                             \n","selfie,                                            | memory,                                           \n","society                                            | mode,                                             \n","soon,                                              | model,                                            \n","store                                              | models,                                           \n","students                                           | models.                                           \n","styles                                             | months,                                           \n","talk,                                              | next,                                             \n","team.                                              | no                                                \n","that.                                              | now,                                              \n","thing.                                             | paper,                                            \n","things,                                            | people,                                           \n","turbo.                                             | problems,                                         \n","up,                                                | products                                          \n","was-                                               | remember,                                         \n","what?                                              | robot,                                            \n","wire                                               | society,                                          \n","wonderful.                                         | soon                                              \n","worry,                                             | students,                                         \n","xy                                                 | styles.                                           \n","years                                              | they'd                                            \n","                                                   | things.                                           \n","                                                   | today,                                            \n","                                                   | understanding.                                    \n","                                                   | use,                                              \n","                                                   | use.                                              \n","                                                   | users,                                            \n","                                                   | vision,                                           \n","                                                   | well                                              \n","                                                   | wire,                                             \n","                                                   | wonderful                                         \n","                                                   | world,                                            \n","                                                   | worry                                             \n","                                                   | yeah                                              \n","                                                   | years,                                            \n"]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyP7CRavXZwzVA6GugLvyUqN"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}