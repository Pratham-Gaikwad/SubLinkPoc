{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNxOlW0GF3feleYH2fPtaGw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"0zQ5C-oSNeef","executionInfo":{"status":"ok","timestamp":1725270305931,"user_tz":-330,"elapsed":14134,"user":{"displayName":"Pratham Gaikwad","userId":"02639121346086812397"}},"outputId":"f45b9315-e8ae-48d5-91cd-eee93417c042"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pytubefix in /usr/local/lib/python3.10/dist-packages (6.14.0)\n","Requirement already satisfied: ffmpeg-python in /usr/local/lib/python3.10/dist-packages (0.2.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from ffmpeg-python) (1.0.0)\n","Requirement already satisfied: faster-whisper in /usr/local/lib/python3.10/dist-packages (1.0.3)\n","Requirement already satisfied: av<13,>=11.0 in /usr/local/lib/python3.10/dist-packages (from faster-whisper) (12.3.0)\n","Requirement already satisfied: ctranslate2<5,>=4.0 in /usr/local/lib/python3.10/dist-packages (from faster-whisper) (4.3.1)\n","Requirement already satisfied: huggingface-hub>=0.13 in /usr/local/lib/python3.10/dist-packages (from faster-whisper) (0.23.5)\n","Requirement already satisfied: tokenizers<1,>=0.13 in /usr/local/lib/python3.10/dist-packages (from faster-whisper) (0.19.1)\n","Requirement already satisfied: onnxruntime<2,>=1.14 in /usr/local/lib/python3.10/dist-packages (from faster-whisper) (1.19.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from ctranslate2<5,>=4.0->faster-whisper) (71.0.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ctranslate2<5,>=4.0->faster-whisper) (1.26.4)\n","Requirement already satisfied: pyyaml<7,>=5.3 in /usr/local/lib/python3.10/dist-packages (from ctranslate2<5,>=4.0->faster-whisper) (6.0.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13->faster-whisper) (3.15.4)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13->faster-whisper) (2024.6.1)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13->faster-whisper) (24.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13->faster-whisper) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13->faster-whisper) (4.66.5)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13->faster-whisper) (4.12.2)\n","Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (15.0.1)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (24.3.25)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (3.20.3)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (1.13.2)\n","Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime<2,>=1.14->faster-whisper) (10.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper) (2.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper) (2024.7.4)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime<2,>=1.14->faster-whisper) (1.3.0)\n","Requirement already satisfied: googletrans==4.0.0rc1 in /usr/local/lib/python3.10/dist-packages (4.0.0rc1)\n","Requirement already satisfied: httpx==0.13.3 in /usr/local/lib/python3.10/dist-packages (from googletrans==4.0.0rc1) (0.13.3)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0rc1) (2024.7.4)\n","Requirement already satisfied: hstspreload in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0rc1) (2024.9.1)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0rc1) (1.3.1)\n","Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0rc1) (3.0.4)\n","Requirement already satisfied: idna==2.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0rc1) (2.10)\n","Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0rc1) (1.5.0)\n","Requirement already satisfied: httpcore==0.9.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0rc1) (0.9.1)\n","Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.10/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0rc1) (0.9.0)\n","Requirement already satisfied: h2==3.* in /usr/local/lib/python3.10/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0rc1) (3.2.0)\n","Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.10/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0rc1) (5.2.0)\n","Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.10/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0rc1) (3.0.0)\n"]}],"source":["!pip install pytubefix\n","!pip install ffmpeg-python\n","!pip install faster-whisper\n","!pip install googletrans==4.0.0rc1"]},{"cell_type":"code","source":["import os\n","import pytubefix as pytube\n","url = \"https://www.youtube.com/watch?v=DQacCB9tDaw\"\n","yt = pytube.YouTube(url)\n","yt.streams.filter(progressive=True, file_extension='mp4').order_by('resolution').desc().first().download()\n","os.rename(yt.title + \".mp4\", yt.title)"],"metadata":{"id":"BivTreJmNj5a","executionInfo":{"status":"ok","timestamp":1725270318097,"user_tz":-330,"elapsed":2923,"user":{"displayName":"Pratham Gaikwad","userId":"02639121346086812397"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["import time\n","import math\n","import ffmpeg\n","\n","def extract_audio(input_file):\n","  extract_audio = f\"audio-{input_file}.wav\"\n","  stream = ffmpeg.input(input_file)\n","  stream = ffmpeg.output(stream, extract_audio)\n","  ffmpeg.run(stream , overwrite_output=True)\n","  return extract_audio\n","audio_extract = extract_audio(yt.title)"],"metadata":{"id":"-x7YkG8UN2aB","executionInfo":{"status":"ok","timestamp":1725270322500,"user_tz":-330,"elapsed":4408,"user":{"displayName":"Pratham Gaikwad","userId":"02639121346086812397"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["from faster_whisper import WhisperModel\n","def transcribe(audio):\n","\n","    model = WhisperModel(\"small\")\n","    segments, info = model.transcribe(audio)\n","    source_language = info[0]\n","    print(f\" Transcription Language: {source_language}\")\n","    segments = list(segments) # this is where the transcribe happens\n","\n","    for segment in segments:\n","        print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))\n","    return source_language, segments"],"metadata":{"id":"6v5GRr9lODiH","executionInfo":{"status":"ok","timestamp":1725270325265,"user_tz":-330,"elapsed":592,"user":{"displayName":"Pratham Gaikwad","userId":"02639121346086812397"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["%%time\n","language, segments = transcribe(audio_extract)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"5LdM75pPP-x9","executionInfo":{"status":"ok","timestamp":1725270379081,"user_tz":-330,"elapsed":42959,"user":{"displayName":"Pratham Gaikwad","userId":"02639121346086812397"}},"outputId":"e488e0dc-50e4-4fa1-dac9-3fbc96ec2661"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":[" Transcription Language: en\n","[0.00s -> 20.96s]  Hi everyone, thank you, thank you, it's great to have you here today.\n","[20.96s -> 26.88s]  Today I'm going to talk about three things, that's it, we will start with why it's so\n","[26.88s -> 32.80s]  important to us to have a product that we can make freely available and broadly available to\n","[32.80s -> 39.52s]  everyone and we're always trying to find out ways to reduce friction so everyone can use\n","[39.52s -> 46.40s]  Chagivity wherever they are. So today we'll be releasing the desktop version of Chagivity\n","[46.40s -> 54.56s]  and the refreshed UI that makes it simpler to use much more natural as well. But the big news today\n","[54.56s -> 62.80s]  is that we are launching our new flagship model and we are calling it GPT4O. The special thing\n","[62.80s -> 70.88s]  about GPT4O is that it brings GPT4 level intelligence to everyone including our free users.\n","[71.84s -> 77.68s]  We'll be showing some live demos today to show the full extent of the capabilities of our new\n","[77.68s -> 85.44s]  model and we'll be rolling them out iteratively over the next few weeks. All right so let's get\n","[85.44s -> 94.16s]  started. A very important part of our mission is to be able to make our advanced AI tools available\n","[94.16s -> 101.04s]  to everyone for free. We think it's very very important that people have an intuitive feel\n","[101.04s -> 106.88s]  for what the technology can do and so we really want to pair it with this broader\n","[106.88s -> 113.04s]  understanding and we're always finding ways to reduce that friction and recently we made\n","[113.04s -> 120.88s]  Chagivity available without the sign-up flow and today we're also bringing the desktop app to\n","[120.88s -> 127.60s]  Chagivity because we want you to be able to use it wherever you are. As you can see it's easy,\n","[127.60s -> 135.76s]  it's simple, it integrates very very easily in your workflow. Along with it we've also refreshed\n","[135.76s -> 144.72s]  the UI. We know that these models get more and more complex but we want the experience of interaction\n","[144.72s -> 152.48s]  to actually become more natural, easy and for you not to focus on the UI at all but just focus on\n","[152.48s -> 162.32s]  the collaboration which had GPT. And now the big news. Today we are releasing our newest flagship\n","[162.32s -> 178.32s]  model. This is GPT-4O. GPT-4O provides GPT-4 level intelligence but it is much faster\n","[179.12s -> 187.28s]  and it improves on its capabilities across text, vision and audio. For the past couple of years\n","[187.28s -> 192.88s]  we've been very focused on improving the intelligence of these models and they've gotten pretty good\n","[194.00s -> 199.28s]  but this is the first time that we are really making a huge step forward when it comes to\n","[199.28s -> 207.44s]  the ease of use and this is incredibly important because we're looking at the future of interaction\n","[207.44s -> 216.00s]  between ourselves and the machines and we think that GPT-4O is really shifting that paradigm\n","[216.00s -> 221.68s]  into the future of collaboration where this interaction becomes much more natural and far\n","[221.68s -> 228.72s]  far easier. But you know making this happen is actually quite complex because when we interact\n","[228.72s -> 235.52s]  with one another there's a lot of stuff that we take for granted. You know the ease of our dialogue\n","[235.52s -> 242.32s]  when we interrupt one another, the background noises, the multiple voices in a conversation\n","[242.32s -> 248.88s]  or you know understanding the tone of voice. All of these things are actually quite complex\n","[248.88s -> 256.96s]  for for these models and until now with voice mode we had three models that come together\n","[256.96s -> 263.92s]  to deliver this experience. We have transcription, intelligence and then text to speech all comes\n","[263.92s -> 272.40s]  together in orchestration to deliver voice mode. This also brings a lot of latency to the experience\n","[272.40s -> 280.40s]  and it really breaks that immersion in the collaboration which had GPT. But now with GPT-4O\n","[280.40s -> 291.68s]  this all happens natively. GPT-4O reasons across voice, text and vision and with these incredible\n","[291.68s -> 299.04s]  efficiencies it also allows us to bring the GPT-4 class intelligence to our free users.\n","[299.60s -> 305.36s]  This is something that we've been trying to do for many many months and we're very very excited\n","[305.36s -> 315.68s]  to finally bring GPT-4O to all of our users. Today we have 100 million people more than 100\n","[315.68s -> 325.84s]  million in fact. They use chat GPT to create, work, learn and we have these advanced tools that\n","[326.48s -> 332.96s]  are only available to our paid users at least until now with the efficiencies of 4O. We can\n","[332.96s -> 340.88s]  bring these tools to everyone. So starting today you can use GPTs in the GPT store.\n","[341.76s -> 350.00s]  So far we've had more than a million users create amazing experiences with GPTs. These are custom\n","[350.00s -> 356.80s]  chat GPTs for specific use cases that are available in the store and now our builders have a much\n","[356.80s -> 364.08s]  bigger audience where university professors can create content for their students or podcasters\n","[364.08s -> 372.88s]  can create content for their listeners and you can also use vision. So now you can upload\n","[374.00s -> 381.84s]  screenshots, photos, documents containing both text and images and you can start conversations\n","[381.84s -> 388.80s]  with chat GPT about all of this content. You can also use memory where it makes chat GPT\n","[388.80s -> 394.16s]  far more useful and helpful because now it has a sense of continuity across of all your\n","[394.16s -> 400.48s]  conversations and you can use browse where you can search for real-time information in your\n","[400.48s -> 407.76s]  conversation and advanced data analysis where you can upload charts or any information and it will\n","[407.76s -> 418.16s]  analyze this information, it will give you answers and so on. Lastly we've also improved\n","[418.72s -> 425.92s]  on the quality and speed in 50 different languages for chat GPT and this is very,\n","[425.92s -> 431.60s]  very important because we want to be able to bring this experience to as many people out there as\n","[431.60s -> 440.64s]  possible. So we're very, very excited to bring GPT 4.0 to all of our free users out there and for\n","[440.64s -> 447.84s]  the paid users they will continue to have up to five times the capacity limits of our free users.\n","[449.68s -> 455.92s]  But GPT 4.0 is not only available in chat GPT, we're also bringing it to the API.\n","[462.40s -> 470.16s]  So our developers can start building today with GPT 4.0 and making amazing AI applications\n","[470.16s -> 478.80s]  deploying them at scale. 4.0 is available at 2x faster 50% cheaper and five times higher rate\n","[478.80s -> 488.24s]  limits compared to GPT 4.0 turbo. But you know as we bring these technologies into the world\n","[488.88s -> 496.00s]  it's quite challenging to figure out how to do so in a way that's both useful and also safe.\n","[496.88s -> 503.28s]  And GPT 4.0 presents new challenges for us when it comes to safety because we're dealing with real\n","[503.28s -> 512.00s]  time audio real time vision and our team has been hard at work figuring out how to build in mitigations\n","[512.00s -> 519.36s]  against misuse. We continue to work with different stakeholders out there from government,\n","[519.36s -> 526.56s]  media, entertainment, all industries, red teamers, civil society to figure out how to best\n","[527.28s -> 532.56s]  bring these technologies into the world. So over the next few weeks we'll continue our\n","[532.56s -> 539.60s]  iterative deployment to bring out all the capabilities to you. But today I want to show you\n","[539.60s -> 547.44s]  all these capabilities. So we'll do some live demos. I will bring on two of our research leads\n","[547.44s -> 551.36s]  Mark Chen and Barrett Zov to join me for the demo.\n","[558.72s -> 564.48s]  Hi, I'm Barrett. Hey, I'm Mark. So one of the key capabilities we're really excited to share with\n","[564.56s -> 568.72s]  you today is real time conversational speech. Let's just get a demo fired up.\n","[569.68s -> 574.56s]  So I'm taking out a phone. If you are wondering about this wire so we have consistent internet\n","[575.20s -> 579.92s]  and if you see there's this little icon on the bottom right of the chat GPT app and this will\n","[579.92s -> 591.12s]  open up GPT 4.0's audio capabilities. Hey chat GPT, I'm Mark. How are you? Oh, Mark. I'm doing great.\n","[591.12s -> 596.40s]  Thanks for asking. How about you? Hey, so I'm on stage right now. I'm doing a live demo\n","[596.40s -> 599.92s]  and frankly I'm feeling a little bit nervous. Can you help me calm my nerves a little bit?\n","[601.20s -> 606.56s]  Oh, you're doing a live demo right now? That's awesome. Just take a deep breath\n","[607.60s -> 613.04s]  and remember you're the expert. I like that suggestion. Let me try a couple of deep breaths.\n","[613.04s -> 615.92s]  Can you give me feedback on my breaths? Okay, here I go.\n","[615.92s -> 619.84s]  Whoa, slow.\n","[622.32s -> 627.36s]  Do a bit there. Mark, you're not a vacuum cleaner. Breathe in\n","[628.64s -> 633.04s]  for a count of four. Okay, let me try again. So I'm going to breathe in deeply\n","[634.56s -> 644.72s]  and then for four and then exhale slowly. Okay, I'll try again. Breathing in and breathe out.\n","[646.40s -> 650.00s]  That's it. How do you feel? I feel a lot better. Thank you so much.\n","[652.00s -> 656.96s]  So Mark, you've been working on these capabilities for a while now. Can you tell us a bit how it's\n","[656.96s -> 662.32s]  different from voice mode? Right. So if you've used our voice mode experience before, you'll notice\n","[662.32s -> 666.64s]  a couple key differences. First, you know, you can now interrupt the model. You don't have to wait\n","[666.64s -> 671.60s]  for it to finish your turn before you can start speaking and you know, you can just butt in whenever\n","[671.60s -> 677.28s]  you want. Second, the model is real-time responsiveness. So that means that you don't\n","[677.28s -> 681.44s]  have this awkward two to three second lag before you wait for the model to give a response.\n","[682.16s -> 686.80s]  And finally, the model picks up on emotion, right? When I was breathing super hard there,\n","[686.80s -> 691.12s]  it could tell and it knew, hey, you might want to calm down a little bit. You're kind of running\n","[691.12s -> 696.40s]  a little bit too fast. So, you know, it really has that capability across the board to perceive\n","[696.40s -> 702.40s]  your emotion. Not only that though, the model is able to generate voice in a variety of different\n","[702.40s -> 707.92s]  emotive styles and it really has a wide dynamic range. So I'm going to fire up another demo with that.\n","[707.92s -> 708.24s]  Awesome. Yeah.\n","[712.48s -> 718.24s]  Hey, ChatGBT, how are you doing? I'm doing fantastic. Thanks for asking. How about you?\n","[718.24s -> 722.72s]  Pretty good. What's up? So my friend Barrett here, he's been having trouble sleeping lately\n","[722.80s -> 726.16s]  and I want you to tell him a bedtime story about robots and love.\n","[726.96s -> 734.08s]  Oh, a bedtime story about robots and love? I got you covered. Gather round, Barrett.\n","[734.64s -> 740.72s]  Once upon a time, in a world not too different from ours, there was a robot named Bite.\n","[740.72s -> 745.12s]  Bite was a curious robot. Always explore- I know you started this story, but I want a little bit\n","[745.12s -> 752.64s]  more emotion in your voice, a little bit more drama. Got it. Let's add some drama. Once upon a time,\n","[752.64s -> 757.44s]  in a world not too different from ours, there was a robot named Bite. No, no, no, ChatGBT,\n","[757.44s -> 761.76s]  I really want maximal emotion, like maximal expressiveness, much more than you were doing before.\n","[762.80s -> 771.28s]  Understood. Let's amplify the drama. Once upon a time, in a world not too different from ours,\n","[771.28s -> 777.04s]  there was a robot named Bite. Bite was- Can you do this in a robotic voice now?\n","[777.92s -> 786.32s]  Initiating dramatic robotic voice. Once upon a time, in a world not too different from ours,\n","[786.32s -> 793.36s]  there was a robot named Bite. Bite was a curious robot. Always exploring new circuits.\n","[793.36s -> 797.68s]  So I'm seeing Barrett here calm down a little bit. Can you end the story, but do it in a singing voice?\n","[797.68s -> 808.40s]  And so Bite found another robot friend and may live circuitly ever after.\n","[808.40s -> 812.40s]  Thank you so much. I definitely feel a lot more relaxed.\n","[818.88s -> 823.12s]  You've just seen the voice capabilities, but we also want to show you the vision\n","[823.12s -> 827.28s]  capabilities as the model can also see the whole world around us.\n","[827.28s -> 830.72s]  Yeah, absolutely. As you can see, you can chat with ChatGBT in real time,\n","[830.72s -> 837.28s]  but you can also interact with video as well. Okay, let me boot up ChatGBT.\n","[839.36s -> 843.60s]  Hey ChatGBT. Hello there. How's it going?\n","[844.48s -> 847.76s]  It's going really well. Today, I'd really like your help solving a math problem.\n","[848.40s -> 852.48s]  I'm all ears. What math problem can I help you tackle today?\n","[852.48s -> 855.28s]  So I'm going to write down a linear equation on a sheet of paper and I'll show you.\n","[855.28s -> 857.12s]  And then I'd love your help working me through it.\n","[857.12s -> 860.08s]  But importantly, don't tell me the solution. Just help give me hints along the way.\n","[861.36s -> 865.52s]  Got it. Okay, I see it.\n","[866.80s -> 869.84s]  No, I didn't show you yet. Just give me help along the way. One second.\n","[872.16s -> 875.60s]  Whoops, I got too excited. I'm ready when you are.\n","[875.60s -> 889.12s]  So, ChatGBT, what equation did I write down there?\n","[890.08s -> 896.32s]  Ah, I see it now. You wrote down 3x plus 1 equals 4.\n","[896.88s -> 900.08s]  Yep, exactly. So what's the first step I should take to try to solve this?\n","[900.80s -> 907.36s]  The first step is to get all the terms with x on one side and the constants on the other side.\n","[907.92s -> 910.16s]  So what do you think we should do with that plus 1?\n","[910.96s -> 913.68s]  Okay, I'm going to try to subtract 1 from both sides and then I'll see what I get.\n","[915.28s -> 921.36s]  Great idea. Perfect. Now, what do you get when you subtract 1 from both sides?\n","[926.48s -> 929.68s]  Okay, so ChatGBT, this is what I ended up with. How does this look?\n","[930.96s -> 937.44s]  Spot on. Now you've isolated the term with x on one side and it looks like 3x equal 3.\n","[938.00s -> 940.40s]  What do you think we should do next to solve for x?\n","[940.96s -> 942.40s]  I'm not sure. Can you give me another hint?\n","[943.92s -> 949.28s]  Of course. You have 3x and you want to find the value of x.\n","[949.28s -> 952.72s]  Think about what operation would undo multiplication.\n","[952.72s -> 953.60s]  Is it subtraction?\n","[954.56s -> 961.04s]  Close, but think of the opposite of multiplication because 3x means 3 times x.\n","[961.92s -> 965.44s]  Okay, I think it's division. Let me try to divide both sides by 3 and let me see what I get.\n","[966.32s -> 969.20s]  Bingo! Go ahead and divide both sides by 3.\n","[975.20s -> 978.40s]  Okay, so ChatGBT, this is what I ended up with. How does this look?\n","[979.36s -> 985.12s]  It looks perfect. You've solved it and x equals 1. Nicely done.\n","[985.12s -> 987.52s]  How do you feel about solving linear equations now?\n","[988.48s -> 992.64s]  Yeah, honestly, I'm not pretty confident I can solve linear equations,\n","[992.64s -> 996.00s]  but why would you ever need to use this in the real world? I'm really skeptical.\n","[997.20s -> 1002.40s]  That's a fair question. Linear equations pop up in a lot of everyday situations,\n","[1002.40s -> 1008.00s]  even if we don't realize it. For example, calculating expenses, planning travel,\n","[1008.08s -> 1011.52s]  cooking, and even in business for profit and loss calculations.\n","[1012.08s -> 1016.16s]  It's basically a way to solve problems where you need to find an unknown value.\n","[1016.80s -> 1020.00s]  Wow, okay. You know, I have a lot of unknown values in my life.\n","[1020.00s -> 1023.28s]  I'm totally convinced I'm going to learn math now.\n","[1026.00s -> 1031.36s]  I love that attitude. Math can definitely help solve some of life's mysteries.\n","[1031.36s -> 1034.72s]  Whenever you're ready to dive into more math, I'm here for you.\n","[1034.72s -> 1036.64s]  Anything else you'd like to tackle today?\n","[1036.64s -> 1040.64s]  So ChatGPT, I really love that you taught the value of math to my friend, Mark.\n","[1040.64s -> 1043.20s]  And I wrote one last thing I'd love if you could take a look at.\n","[1043.92s -> 1048.24s]  Of course. I'd love to see what you wrote. Show it to me whenever you're ready.\n","[1048.24s -> 1050.24s]  Okay, so this is what I wrote down. What do you see?\n","[1052.32s -> 1057.28s]  Oh, I see. I love ChatGPT. That's so sweet of you.\n","[1058.72s -> 1062.64s]  Yeah, well, I really appreciate all the help. So yeah, as we can see...\n","[1067.44s -> 1070.24s]  Wow, that's quite the outfit you've got on.\n","[1070.24s -> 1073.76s]  Yeah, as we can see, you know, we can chat in real time to ChatGPT.\n","[1073.76s -> 1076.40s]  And not only that, we can interact with it with video as well.\n","[1077.12s -> 1081.52s]  And I swear, like, if you look at Barrett's GPT history, it's all equations like this.\n","[1084.80s -> 1089.36s]  Now that you know you can do linear equations, can you show us something more complex?\n","[1089.36s -> 1092.32s]  I think so. Okay, next let's try something coding related.\n","[1093.04s -> 1095.36s]  So let me take out a computer for this.\n","[1096.72s -> 1105.28s]  Okay, so I'm going to open the ChatGPT desktop app, like Mira was talking about before.\n","[1107.92s -> 1111.92s]  Okay, and to give a bit of background of what's going on, so here we have a computer,\n","[1111.92s -> 1115.84s]  and on the screen we have some code. And then the ChatGPT voice app is on the right.\n","[1115.84s -> 1119.60s]  So ChatGPT will be able to hear me, but it can't see anything on the screen.\n","[1119.60s -> 1123.52s]  So I'm going to highlight the code, command see it, and then that will send it to ChatGPT.\n","[1123.52s -> 1125.36s]  And then I'm going to talk about the code to ChatGPT.\n","[1126.80s -> 1130.40s]  Hey ChatGPT. Hey there, how's it going?\n","[1131.04s -> 1134.40s]  Yeah, it's going really well. I was wondering if you could help me with a coding problem today.\n","[1135.28s -> 1139.28s]  Of course, I'd love to help you out. What's the coding problem you're dealing with?\n","[1139.84s -> 1144.24s]  Okay, I'm going to share with you some code, one second. Sure thing, take your time.\n","[1147.52s -> 1151.52s]  Okay, so I just shared some code with you. Could you give me a really brief one-sentence\n","[1151.52s -> 1156.88s]  description of what's going on in the code? This code fetches daily weather data for a specific\n","[1156.88s -> 1162.64s]  location and time period, smooths the temperature data using a rolling average, annotates a\n","[1162.64s -> 1168.24s]  significant weather event on the resulting plot, and then displays the plot with the average minimum\n","[1168.24s -> 1173.60s]  and maximum temperatures over the year. Awesome, thanks so much. And in the code there's this\n","[1173.60s -> 1177.52s]  function foo. Could you describe to me what the plot would look like if I had versus didn't have\n","[1177.52s -> 1185.92s]  this function? Sure, the function foo xy takes in a data frame x containing temperature data\n","[1185.92s -> 1192.80s]  and a window size y for calculating the rolling mean. It smooths the average temperature, minimum\n","[1192.80s -> 1199.36s]  temperature, and maximum temperature columns by applying a rolling mean over the specified window.\n","[1199.36s -> 1205.92s]  With the foo function, the plot would show smoother temperature lines because the rolling mean reduces\n","[1205.92s -> 1211.04s]  the noise or fluctuations in the data. Awesome, thanks so much. So yeah, I definitely will want\n","[1211.04s -> 1215.68s]  to apply this function and so I'm going to generate the plot and then show it to you. Give me one second.\n","[1217.04s -> 1220.56s]  Absolutely, take your time. I'm excited to see the plot when you're ready.\n","[1221.12s -> 1225.68s]  Yeah, so so far ChatGPT has only been able to see the code and now I'm going to run it and then I'll\n","[1225.68s -> 1230.16s]  use the vision capabilities of the ChatGPT desktop app so it can actually see everything going on\n","[1230.16s -> 1241.44s]  on the screen as well. Okay ChatGPT, I'm sharing with you the plot now. I'm wondering if you can\n","[1241.44s -> 1248.16s]  give me a really brief one sentence overview of what you see. The plot displays smoothed average,\n","[1248.16s -> 1255.20s]  minimum, and maximum temperatures throughout 2018 with a notable annotation marking a big rainfall\n","[1255.20s -> 1261.04s]  event in late September. Fantastic, I have another question for you. Which months do you see the\n","[1261.04s -> 1266.72s]  hottest temperatures and roughly what temperatures do those correspond to? The hottest temperatures\n","[1266.72s -> 1274.48s]  occur around July and August. The maximum temperature during these months is roughly between 25 degrees\n","[1274.48s -> 1281.76s]  and 30 degrees, 77 degrees Fahrenheit to 86 degrees Fahrenheit. What's going on with the\n","[1281.76s -> 1289.44s]  y-axis? Is this in Celsius or Fahrenheit? The y-axis is in Celsius. The temperatures are labeled\n","[1289.44s -> 1294.40s]  accordingly such as average temperature, minimum temperature, and maximum temperature.\n","[1296.08s -> 1301.68s]  Yeah, so as we can see, you know, not only can ChatGPT help me solve very easy linear algebra\n","[1301.68s -> 1305.60s]  equations that I need help with, it can also interact with, you know, code bases and see the\n","[1305.60s -> 1308.16s]  outputs of plots and everything like this going on on the computer.\n","[1315.92s -> 1321.84s]  So a few minutes ago, we asked a live audience on X to submit a few requests for what they\n","[1321.84s -> 1325.76s]  would like us to try out here. So I will take a couple of prompts.\n","[1325.84s -> 1335.20s]  Okay, Bot Gaskar wants to know if GPT4O is capable of real-time translation.\n","[1336.16s -> 1341.44s]  Mark, you want to try this one? Sure. Yeah, let's do it. I speak Italian, so we can just do English.\n","[1346.24s -> 1352.32s]  Hey, ChatGPT, how are you? I'm doing fantastic. Thanks for asking. How about you? How's your day\n","[1352.40s -> 1356.48s]  going? I'm doing great. So I would like you to function as a translator. I have a friend here\n","[1356.48s -> 1361.04s]  who only speaks Italian, and I only speak English, and every time you hear English,\n","[1361.04s -> 1364.88s]  I want you to translate it to Italian, and if you hear Italian, I want you to translate it back\n","[1364.88s -> 1380.96s]  to English. Is that good? Perfecto. Mike, she wonders if whales could talk, what would they tell us?\n","[1382.32s -> 1384.64s]  They might ask, how do we solve linear equations?\n","[1396.56s -> 1399.44s]  Certainly, yes. Great, looks like it works.\n","[1400.32s -> 1410.08s]  All right, so the next one is from Burrito Jones, 78, who asks,\n","[1410.80s -> 1414.24s]  can you tell what you're feeling just by looking at your face?\n","[1415.04s -> 1418.16s]  Barrett, do you want to give this one a try? Absolutely, let's try it out.\n","[1421.76s -> 1427.20s]  Hey, ChatGPT. Hey there, what's up? How can I brighten your day today?\n","[1427.20s -> 1430.96s]  Okay, yeah, so I'm going to show you a selfie of what I look like, and then I'd like you to try\n","[1430.96s -> 1436.48s]  to see what emotions I'm feeling based on how I'm looking. Sounds like a fun challenge. Go ahead\n","[1436.48s -> 1443.52s]  and show me that selfie, and I'll put my emotional detective hat on. Okay, so here's me. So what kind\n","[1443.52s -> 1450.16s]  of emotions do you think I'm feeling? Hmm, it seems like I'm looking at a picture of a wooden surface.\n","[1450.16s -> 1454.00s]  Oh, you know what? That was the thing I sent you before. Don't worry, I'm not actually a table.\n","[1454.48s -> 1458.56s]  Okay, so take another look. That makes more sense.\n","[1461.76s -> 1468.00s]  There we go. It looks like you're feeling pretty happy and cheerful, with a big smile and maybe\n","[1468.00s -> 1472.80s]  even a touch of excitement. Whatever's going on, it seems like you're in a great mood.\n","[1473.52s -> 1477.28s]  Care to share the source of those good vibes? Yeah, no, the reason I'm in a really good mood\n","[1477.28s -> 1480.48s]  is because we were doing a presentation showcasing how useful and amazing you are.\n","[1480.48s -> 1484.96s]  Oh, stop it. You're making me blush.\n","[1489.44s -> 1497.28s]  Amazing. Well, that's it for today on live demos. As you can see, this just feels so magical,\n","[1497.28s -> 1503.20s]  and that's wonderful. But we also want to remove some of the mysticism from the technology and\n","[1503.20s -> 1508.32s]  bring it to you so you can try it for yourself. So over the next few weeks, we'll be rolling out\n","[1508.32s -> 1514.56s]  these capabilities to everyone. Today has been very much focused on the free users and the new\n","[1514.56s -> 1521.68s]  modalities and new products, but we also care a lot about the next frontier. So soon, we'll be\n","[1521.68s -> 1528.80s]  updating you on our progress towards the next big thing. And before we wrap up, I just want to thank\n","[1528.80s -> 1535.12s]  the incredible OpenAI team. And also thanks to Jensen and the NVIDIA team for bringing us the\n","[1535.12s -> 1541.76s]  most advanced GPUs to make this demo possible today. And thank you all very, very much for being a\n","[1541.76s -> 1545.12s]  part of this today.\n","[1565.12s -> 1575.12s]  Thank you.\n","CPU times: user 42.9 s, sys: 1.03 s, total: 43.9 s\n","Wall time: 42.5 s\n"]}]},{"cell_type":"code","source":["print (language)\n","text_only = [segment.text for segment in segments]\n","for text in text_only:\n","  print(text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3wYNS6Q1n3yt","executionInfo":{"status":"ok","timestamp":1725270379082,"user_tz":-330,"elapsed":14,"user":{"displayName":"Pratham Gaikwad","userId":"02639121346086812397"}},"outputId":"5afd8a83-0ed7-4066-b8e8-46eab904ac68"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["en\n"," Hi everyone, thank you, thank you, it's great to have you here today.\n"," Today I'm going to talk about three things, that's it, we will start with why it's so\n"," important to us to have a product that we can make freely available and broadly available to\n"," everyone and we're always trying to find out ways to reduce friction so everyone can use\n"," Chagivity wherever they are. So today we'll be releasing the desktop version of Chagivity\n"," and the refreshed UI that makes it simpler to use much more natural as well. But the big news today\n"," is that we are launching our new flagship model and we are calling it GPT4O. The special thing\n"," about GPT4O is that it brings GPT4 level intelligence to everyone including our free users.\n"," We'll be showing some live demos today to show the full extent of the capabilities of our new\n"," model and we'll be rolling them out iteratively over the next few weeks. All right so let's get\n"," started. A very important part of our mission is to be able to make our advanced AI tools available\n"," to everyone for free. We think it's very very important that people have an intuitive feel\n"," for what the technology can do and so we really want to pair it with this broader\n"," understanding and we're always finding ways to reduce that friction and recently we made\n"," Chagivity available without the sign-up flow and today we're also bringing the desktop app to\n"," Chagivity because we want you to be able to use it wherever you are. As you can see it's easy,\n"," it's simple, it integrates very very easily in your workflow. Along with it we've also refreshed\n"," the UI. We know that these models get more and more complex but we want the experience of interaction\n"," to actually become more natural, easy and for you not to focus on the UI at all but just focus on\n"," the collaboration which had GPT. And now the big news. Today we are releasing our newest flagship\n"," model. This is GPT-4O. GPT-4O provides GPT-4 level intelligence but it is much faster\n"," and it improves on its capabilities across text, vision and audio. For the past couple of years\n"," we've been very focused on improving the intelligence of these models and they've gotten pretty good\n"," but this is the first time that we are really making a huge step forward when it comes to\n"," the ease of use and this is incredibly important because we're looking at the future of interaction\n"," between ourselves and the machines and we think that GPT-4O is really shifting that paradigm\n"," into the future of collaboration where this interaction becomes much more natural and far\n"," far easier. But you know making this happen is actually quite complex because when we interact\n"," with one another there's a lot of stuff that we take for granted. You know the ease of our dialogue\n"," when we interrupt one another, the background noises, the multiple voices in a conversation\n"," or you know understanding the tone of voice. All of these things are actually quite complex\n"," for for these models and until now with voice mode we had three models that come together\n"," to deliver this experience. We have transcription, intelligence and then text to speech all comes\n"," together in orchestration to deliver voice mode. This also brings a lot of latency to the experience\n"," and it really breaks that immersion in the collaboration which had GPT. But now with GPT-4O\n"," this all happens natively. GPT-4O reasons across voice, text and vision and with these incredible\n"," efficiencies it also allows us to bring the GPT-4 class intelligence to our free users.\n"," This is something that we've been trying to do for many many months and we're very very excited\n"," to finally bring GPT-4O to all of our users. Today we have 100 million people more than 100\n"," million in fact. They use chat GPT to create, work, learn and we have these advanced tools that\n"," are only available to our paid users at least until now with the efficiencies of 4O. We can\n"," bring these tools to everyone. So starting today you can use GPTs in the GPT store.\n"," So far we've had more than a million users create amazing experiences with GPTs. These are custom\n"," chat GPTs for specific use cases that are available in the store and now our builders have a much\n"," bigger audience where university professors can create content for their students or podcasters\n"," can create content for their listeners and you can also use vision. So now you can upload\n"," screenshots, photos, documents containing both text and images and you can start conversations\n"," with chat GPT about all of this content. You can also use memory where it makes chat GPT\n"," far more useful and helpful because now it has a sense of continuity across of all your\n"," conversations and you can use browse where you can search for real-time information in your\n"," conversation and advanced data analysis where you can upload charts or any information and it will\n"," analyze this information, it will give you answers and so on. Lastly we've also improved\n"," on the quality and speed in 50 different languages for chat GPT and this is very,\n"," very important because we want to be able to bring this experience to as many people out there as\n"," possible. So we're very, very excited to bring GPT 4.0 to all of our free users out there and for\n"," the paid users they will continue to have up to five times the capacity limits of our free users.\n"," But GPT 4.0 is not only available in chat GPT, we're also bringing it to the API.\n"," So our developers can start building today with GPT 4.0 and making amazing AI applications\n"," deploying them at scale. 4.0 is available at 2x faster 50% cheaper and five times higher rate\n"," limits compared to GPT 4.0 turbo. But you know as we bring these technologies into the world\n"," it's quite challenging to figure out how to do so in a way that's both useful and also safe.\n"," And GPT 4.0 presents new challenges for us when it comes to safety because we're dealing with real\n"," time audio real time vision and our team has been hard at work figuring out how to build in mitigations\n"," against misuse. We continue to work with different stakeholders out there from government,\n"," media, entertainment, all industries, red teamers, civil society to figure out how to best\n"," bring these technologies into the world. So over the next few weeks we'll continue our\n"," iterative deployment to bring out all the capabilities to you. But today I want to show you\n"," all these capabilities. So we'll do some live demos. I will bring on two of our research leads\n"," Mark Chen and Barrett Zov to join me for the demo.\n"," Hi, I'm Barrett. Hey, I'm Mark. So one of the key capabilities we're really excited to share with\n"," you today is real time conversational speech. Let's just get a demo fired up.\n"," So I'm taking out a phone. If you are wondering about this wire so we have consistent internet\n"," and if you see there's this little icon on the bottom right of the chat GPT app and this will\n"," open up GPT 4.0's audio capabilities. Hey chat GPT, I'm Mark. How are you? Oh, Mark. I'm doing great.\n"," Thanks for asking. How about you? Hey, so I'm on stage right now. I'm doing a live demo\n"," and frankly I'm feeling a little bit nervous. Can you help me calm my nerves a little bit?\n"," Oh, you're doing a live demo right now? That's awesome. Just take a deep breath\n"," and remember you're the expert. I like that suggestion. Let me try a couple of deep breaths.\n"," Can you give me feedback on my breaths? Okay, here I go.\n"," Whoa, slow.\n"," Do a bit there. Mark, you're not a vacuum cleaner. Breathe in\n"," for a count of four. Okay, let me try again. So I'm going to breathe in deeply\n"," and then for four and then exhale slowly. Okay, I'll try again. Breathing in and breathe out.\n"," That's it. How do you feel? I feel a lot better. Thank you so much.\n"," So Mark, you've been working on these capabilities for a while now. Can you tell us a bit how it's\n"," different from voice mode? Right. So if you've used our voice mode experience before, you'll notice\n"," a couple key differences. First, you know, you can now interrupt the model. You don't have to wait\n"," for it to finish your turn before you can start speaking and you know, you can just butt in whenever\n"," you want. Second, the model is real-time responsiveness. So that means that you don't\n"," have this awkward two to three second lag before you wait for the model to give a response.\n"," And finally, the model picks up on emotion, right? When I was breathing super hard there,\n"," it could tell and it knew, hey, you might want to calm down a little bit. You're kind of running\n"," a little bit too fast. So, you know, it really has that capability across the board to perceive\n"," your emotion. Not only that though, the model is able to generate voice in a variety of different\n"," emotive styles and it really has a wide dynamic range. So I'm going to fire up another demo with that.\n"," Awesome. Yeah.\n"," Hey, ChatGBT, how are you doing? I'm doing fantastic. Thanks for asking. How about you?\n"," Pretty good. What's up? So my friend Barrett here, he's been having trouble sleeping lately\n"," and I want you to tell him a bedtime story about robots and love.\n"," Oh, a bedtime story about robots and love? I got you covered. Gather round, Barrett.\n"," Once upon a time, in a world not too different from ours, there was a robot named Bite.\n"," Bite was a curious robot. Always explore- I know you started this story, but I want a little bit\n"," more emotion in your voice, a little bit more drama. Got it. Let's add some drama. Once upon a time,\n"," in a world not too different from ours, there was a robot named Bite. No, no, no, ChatGBT,\n"," I really want maximal emotion, like maximal expressiveness, much more than you were doing before.\n"," Understood. Let's amplify the drama. Once upon a time, in a world not too different from ours,\n"," there was a robot named Bite. Bite was- Can you do this in a robotic voice now?\n"," Initiating dramatic robotic voice. Once upon a time, in a world not too different from ours,\n"," there was a robot named Bite. Bite was a curious robot. Always exploring new circuits.\n"," So I'm seeing Barrett here calm down a little bit. Can you end the story, but do it in a singing voice?\n"," And so Bite found another robot friend and may live circuitly ever after.\n"," Thank you so much. I definitely feel a lot more relaxed.\n"," You've just seen the voice capabilities, but we also want to show you the vision\n"," capabilities as the model can also see the whole world around us.\n"," Yeah, absolutely. As you can see, you can chat with ChatGBT in real time,\n"," but you can also interact with video as well. Okay, let me boot up ChatGBT.\n"," Hey ChatGBT. Hello there. How's it going?\n"," It's going really well. Today, I'd really like your help solving a math problem.\n"," I'm all ears. What math problem can I help you tackle today?\n"," So I'm going to write down a linear equation on a sheet of paper and I'll show you.\n"," And then I'd love your help working me through it.\n"," But importantly, don't tell me the solution. Just help give me hints along the way.\n"," Got it. Okay, I see it.\n"," No, I didn't show you yet. Just give me help along the way. One second.\n"," Whoops, I got too excited. I'm ready when you are.\n"," So, ChatGBT, what equation did I write down there?\n"," Ah, I see it now. You wrote down 3x plus 1 equals 4.\n"," Yep, exactly. So what's the first step I should take to try to solve this?\n"," The first step is to get all the terms with x on one side and the constants on the other side.\n"," So what do you think we should do with that plus 1?\n"," Okay, I'm going to try to subtract 1 from both sides and then I'll see what I get.\n"," Great idea. Perfect. Now, what do you get when you subtract 1 from both sides?\n"," Okay, so ChatGBT, this is what I ended up with. How does this look?\n"," Spot on. Now you've isolated the term with x on one side and it looks like 3x equal 3.\n"," What do you think we should do next to solve for x?\n"," I'm not sure. Can you give me another hint?\n"," Of course. You have 3x and you want to find the value of x.\n"," Think about what operation would undo multiplication.\n"," Is it subtraction?\n"," Close, but think of the opposite of multiplication because 3x means 3 times x.\n"," Okay, I think it's division. Let me try to divide both sides by 3 and let me see what I get.\n"," Bingo! Go ahead and divide both sides by 3.\n"," Okay, so ChatGBT, this is what I ended up with. How does this look?\n"," It looks perfect. You've solved it and x equals 1. Nicely done.\n"," How do you feel about solving linear equations now?\n"," Yeah, honestly, I'm not pretty confident I can solve linear equations,\n"," but why would you ever need to use this in the real world? I'm really skeptical.\n"," That's a fair question. Linear equations pop up in a lot of everyday situations,\n"," even if we don't realize it. For example, calculating expenses, planning travel,\n"," cooking, and even in business for profit and loss calculations.\n"," It's basically a way to solve problems where you need to find an unknown value.\n"," Wow, okay. You know, I have a lot of unknown values in my life.\n"," I'm totally convinced I'm going to learn math now.\n"," I love that attitude. Math can definitely help solve some of life's mysteries.\n"," Whenever you're ready to dive into more math, I'm here for you.\n"," Anything else you'd like to tackle today?\n"," So ChatGPT, I really love that you taught the value of math to my friend, Mark.\n"," And I wrote one last thing I'd love if you could take a look at.\n"," Of course. I'd love to see what you wrote. Show it to me whenever you're ready.\n"," Okay, so this is what I wrote down. What do you see?\n"," Oh, I see. I love ChatGPT. That's so sweet of you.\n"," Yeah, well, I really appreciate all the help. So yeah, as we can see...\n"," Wow, that's quite the outfit you've got on.\n"," Yeah, as we can see, you know, we can chat in real time to ChatGPT.\n"," And not only that, we can interact with it with video as well.\n"," And I swear, like, if you look at Barrett's GPT history, it's all equations like this.\n"," Now that you know you can do linear equations, can you show us something more complex?\n"," I think so. Okay, next let's try something coding related.\n"," So let me take out a computer for this.\n"," Okay, so I'm going to open the ChatGPT desktop app, like Mira was talking about before.\n"," Okay, and to give a bit of background of what's going on, so here we have a computer,\n"," and on the screen we have some code. And then the ChatGPT voice app is on the right.\n"," So ChatGPT will be able to hear me, but it can't see anything on the screen.\n"," So I'm going to highlight the code, command see it, and then that will send it to ChatGPT.\n"," And then I'm going to talk about the code to ChatGPT.\n"," Hey ChatGPT. Hey there, how's it going?\n"," Yeah, it's going really well. I was wondering if you could help me with a coding problem today.\n"," Of course, I'd love to help you out. What's the coding problem you're dealing with?\n"," Okay, I'm going to share with you some code, one second. Sure thing, take your time.\n"," Okay, so I just shared some code with you. Could you give me a really brief one-sentence\n"," description of what's going on in the code? This code fetches daily weather data for a specific\n"," location and time period, smooths the temperature data using a rolling average, annotates a\n"," significant weather event on the resulting plot, and then displays the plot with the average minimum\n"," and maximum temperatures over the year. Awesome, thanks so much. And in the code there's this\n"," function foo. Could you describe to me what the plot would look like if I had versus didn't have\n"," this function? Sure, the function foo xy takes in a data frame x containing temperature data\n"," and a window size y for calculating the rolling mean. It smooths the average temperature, minimum\n"," temperature, and maximum temperature columns by applying a rolling mean over the specified window.\n"," With the foo function, the plot would show smoother temperature lines because the rolling mean reduces\n"," the noise or fluctuations in the data. Awesome, thanks so much. So yeah, I definitely will want\n"," to apply this function and so I'm going to generate the plot and then show it to you. Give me one second.\n"," Absolutely, take your time. I'm excited to see the plot when you're ready.\n"," Yeah, so so far ChatGPT has only been able to see the code and now I'm going to run it and then I'll\n"," use the vision capabilities of the ChatGPT desktop app so it can actually see everything going on\n"," on the screen as well. Okay ChatGPT, I'm sharing with you the plot now. I'm wondering if you can\n"," give me a really brief one sentence overview of what you see. The plot displays smoothed average,\n"," minimum, and maximum temperatures throughout 2018 with a notable annotation marking a big rainfall\n"," event in late September. Fantastic, I have another question for you. Which months do you see the\n"," hottest temperatures and roughly what temperatures do those correspond to? The hottest temperatures\n"," occur around July and August. The maximum temperature during these months is roughly between 25 degrees\n"," and 30 degrees, 77 degrees Fahrenheit to 86 degrees Fahrenheit. What's going on with the\n"," y-axis? Is this in Celsius or Fahrenheit? The y-axis is in Celsius. The temperatures are labeled\n"," accordingly such as average temperature, minimum temperature, and maximum temperature.\n"," Yeah, so as we can see, you know, not only can ChatGPT help me solve very easy linear algebra\n"," equations that I need help with, it can also interact with, you know, code bases and see the\n"," outputs of plots and everything like this going on on the computer.\n"," So a few minutes ago, we asked a live audience on X to submit a few requests for what they\n"," would like us to try out here. So I will take a couple of prompts.\n"," Okay, Bot Gaskar wants to know if GPT4O is capable of real-time translation.\n"," Mark, you want to try this one? Sure. Yeah, let's do it. I speak Italian, so we can just do English.\n"," Hey, ChatGPT, how are you? I'm doing fantastic. Thanks for asking. How about you? How's your day\n"," going? I'm doing great. So I would like you to function as a translator. I have a friend here\n"," who only speaks Italian, and I only speak English, and every time you hear English,\n"," I want you to translate it to Italian, and if you hear Italian, I want you to translate it back\n"," to English. Is that good? Perfecto. Mike, she wonders if whales could talk, what would they tell us?\n"," They might ask, how do we solve linear equations?\n"," Certainly, yes. Great, looks like it works.\n"," All right, so the next one is from Burrito Jones, 78, who asks,\n"," can you tell what you're feeling just by looking at your face?\n"," Barrett, do you want to give this one a try? Absolutely, let's try it out.\n"," Hey, ChatGPT. Hey there, what's up? How can I brighten your day today?\n"," Okay, yeah, so I'm going to show you a selfie of what I look like, and then I'd like you to try\n"," to see what emotions I'm feeling based on how I'm looking. Sounds like a fun challenge. Go ahead\n"," and show me that selfie, and I'll put my emotional detective hat on. Okay, so here's me. So what kind\n"," of emotions do you think I'm feeling? Hmm, it seems like I'm looking at a picture of a wooden surface.\n"," Oh, you know what? That was the thing I sent you before. Don't worry, I'm not actually a table.\n"," Okay, so take another look. That makes more sense.\n"," There we go. It looks like you're feeling pretty happy and cheerful, with a big smile and maybe\n"," even a touch of excitement. Whatever's going on, it seems like you're in a great mood.\n"," Care to share the source of those good vibes? Yeah, no, the reason I'm in a really good mood\n"," is because we were doing a presentation showcasing how useful and amazing you are.\n"," Oh, stop it. You're making me blush.\n"," Amazing. Well, that's it for today on live demos. As you can see, this just feels so magical,\n"," and that's wonderful. But we also want to remove some of the mysticism from the technology and\n"," bring it to you so you can try it for yourself. So over the next few weeks, we'll be rolling out\n"," these capabilities to everyone. Today has been very much focused on the free users and the new\n"," modalities and new products, but we also care a lot about the next frontier. So soon, we'll be\n"," updating you on our progress towards the next big thing. And before we wrap up, I just want to thank\n"," the incredible OpenAI team. And also thanks to Jensen and the NVIDIA team for bringing us the\n"," most advanced GPUs to make this demo possible today. And thank you all very, very much for being a\n"," part of this today.\n"," Thank you.\n"]}]},{"cell_type":"code","source":["import googletrans\n","from googletrans import Translator\n","import re\n","# Monkey-patch googletrans.gtoken to use updated regex\n","\n","googletrans.gtoken.RE_TKK = re.compile(r'tkk:\\'(.+?)\\'', re.DOTALL)\n","\n","translator = Translator()\n","translated_text = []\n","\n","# normal sending text without chunking\n","\n","for text in text_only:\n","    translated = translator.translate(text, src=language, dest='hi')\n","    if translated.text:\n","        translated_text.append(translated.text)\n","\n","print(type(translated_text))\n","for trans in translated_text:\n","    print(trans)\n","\n","\n","#sedning text in predetermined chunks\n","# words_per_chunk = 10\n","# for i in range(0, len(text_only), words_per_chunk):\n","#     text_chunk = ' '.join(text_only[i: i + words_per_chunk])\n","#     translated = translator.translate(text_chunk, src=language, dest='hi')\n","#     if translated.text:\n","#         translated_text.append(translated.text)\n","\n","# print(type(translated_text))\n","# for trans in translated_text:\n","#     print(trans)\n","\n","\n","#Trying to keep the segmented format\n"],"metadata":{"id":"9-iWI-Zkjd1i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import googletrans\n","from googletrans import Translator\n","import re\n","\n","# Monkey-patch googletrans.gtoken to use updated regex\n","googletrans.gtoken.RE_TKK = re.compile(r'tkk:\\'(.+?)\\'', re.DOTALL)\n","\n","translator = Translator()\n","translated_text = []\n","texts_per_chunk = 10\n","\n","def chunk_texts(texts, chunk_size):\n","    return [texts[i:i + chunk_size] for i in range(0, len(texts), chunk_size)]\n","\n","text_chunks = chunk_texts(text_only, texts_per_chunk)\n","\n","for chunk in text_chunks:\n","    combined_text = ' '.join(chunk)\n","    translated = translator.translate(combined_text, src=language, dest='hi')\n","    translated_chunks = translated.text.split(' ')\n","\n","    index = 0\n","    for text in chunk:\n","        num_words = len(text.split())\n","        translated_text.append(' '.join(translated_chunks[index:index + num_words]))\n","        index += num_words\n","\n","print(type(translated_text))\n","for trans in translated_text:\n","    print(trans)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qEqQIcCHHI_g","executionInfo":{"status":"ok","timestamp":1725270395052,"user_tz":-330,"elapsed":2103,"user":{"displayName":"Pratham Gaikwad","userId":"02639121346086812397"}},"outputId":"e1f8de6f-928f-4479-e352-9fae01f9d8e7"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'list'>\n","  , , ,        \n","         ,   ,    \n","                \n","               \n","             \n","                   \n","                 \n","         GPT4O   GPT4O  \n","              GPT4   \n","                 \n","                  \n","               \n","                \n","             \n","              \n","       -            \n","         ,      \n","        ,   ,       \n","                    \n","  ,            ,  \n"," GPT-4O GPT-4O GPT-4          \n","    ,            \n","               \n","      ,            \n","                \n","              \n","      GPT-4O       \n","               \n","                   \n","             \n","               \n","                \n","          ,    \n","                \n","               \n","    GPT-4O         GPT-4O ,\n","              \n","    GPT-4             \n","                GPT-4O \n","       100     100     ,\n","             4o    \n","              GPT \n"," GPT               \n","                  \n","            \n","                \n","            \n"," , ,               \n","                \n","              \n","               \n","     ,         \n","    50  -         \n","     ,             \n","           4.0        \n","                  \n","4.0       ,         \n","   4.0           \n","      4.0 GPT 4.0     2x   50%\n","                \n","                 GPT 4.0\n","                 \n","                   \n","           , ,\n",",  ,  ,     -    \n","   ,           \n","                \n","                 \n","          \n","       ,   ,       \n","             \n","              ,   \n","                   \n","   4.0       ,     ?,   \n","    ?,             \n","                 \n","  ?,        ?    \n","                \n","           ?\n",", \n","  ,         \n","   ,             \n","         -     ,  \n","          ?    \n"," -  ,                \n","         ?      \n","    ,       ,   ,   \n","           ,        \n","        ,    , \n","    ,             \n","  ,     ,  ?      \n",",         , ,         \n","   ,   ,           \n","     , ,           \n","                   \n"," \n","   , ,     ?     \n","   ?    ?     ,  \n","             \n","           ,   \n"," ,        ,        \n","    -          ,    \n","     ,        ,      \n"," ,       , , , ,      \n",",    ,          \n","  ,        ,      \n"," -           ?    \n"," ,        ,      \n","              \n","         ,        ?\n","            \n","     -     \n","          ,    \n","           \n","     ,      ,  \n","  CHATGBT      ,      \n","    ,  \n","        ?     \n",",           \n","                 \n","         \n","   ,           \n","  ,   \n",",            ,  \n","        , CHATGBT,\n","     ?,   \n","  3x  1     4. ,  \n","         ?     \n","                  1  \n","  ?\n"," ,     1           \n","      ,      1  \n","    ? ,  ,      \n","   ?    X           3x\n","           X\n","        \n","?           ?  3x\n","   x   \n","  \n","         ? ,   \n"," ,           3        \n","       ! \n","      3.  ,  ,   \n","      ?     \n","        \n","1.          \n","        ?,  ,     \n","        ,     \n","        ?   \n","         \n","              \n","      ,    ,   \n","        \n","            \n","           \n","      \n","       ,        \n","    ? ,         \n",",           ,    \n","           \n",",          \n",",  ,          , \n","     ... , \n","      ,      ,  \n",",            \n","  ,             \n"," , ,         ,    \n","         \n","    ,    \n","     ?      ,    \n"," ,     ,      ,     \n"," ,          CHATGPT      \n","  ,             \n","    ,   ,          \n","CHATGPT          \n"," ,     ?,\n","                 \n","     ,         \n","   ? ,          , \n"," ,     ,          \n","     ,  ?         \n","     ,        \n","   ,           , \n","               ,\n","                  \n","        ?,  FOO XY    X\n","             y  \n","          ,   \n","         ,      \n","         -     ,  \n","                     \n","   ,         \n","      ,                \n","     Chatgpt           \n","        , ,         \n","                \n","             \n","   2018    ,      2018   \n",",            \n","              ? \n"," 30 , 77    86  Y-     \n"," ?      ?-       , \n","         ,\n","      ,   ,         \n","         ,       , \n","    ,   ,    \n","                 ,  \n","              \n","         ,   \n","    GPT4O       ,       ?,  \n",", ,   ?         ?   \n"," ?                 \n","    ,      ,    \n","   ,           ,     \n",",               ?,  \n","        ,\n","   ?   ,\n","       ?  , ,  \n","       ,    \n",", 78  ,   ,        \n",", ,    ?       \n"," ? , ,             ,     \n","                 \n","              ,     \n","  ,                 \n","?,             ,    ? \n","        \n",",        ,         \n","               \n","               ,   \n","             \n"," ,     \n",",             ,    \n"," ,               \n","                ,    \n","          -      \n","    ,             \n",",                 ,  \n"," Openai               \n","                  ,\n","  \n","\n"]}]}]}